{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import *\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1 获取文本语料库\\n一个文本语料库是一大段文本\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1 获取文本语料库\n",
    "一个文本语料库是一大段文本\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "古腾堡语料库\n",
    "\n",
    "要用Python 解释器加载NLTK 包，然后尝试nltk.corpus.gutenberg.fileids()\n",
    "'''\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "#挑选这些文本的第一个——简·奥斯丁的《爱玛》——并给它一个简短的名称emma，然后找出它包含多少个词\n",
    "\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)\n",
    "# type(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n注意\\n\\n在第1章中，我们演示了如何使用text1.concordance()命令对text1这样的文本进行索引。\\n然而，这是假设你正在使用由from nltk.book import *导入的9 个文本之一。\\n现在你开始研究nltk.corpus中的数据，像前面的例子一样，\\n你必须采用以下语句对来处理索引和第1章中的其它任务：\\n\\nemma = nltk.Text(nltk.corpus.gutenberg.words(\\'austen-emma.txt\\'))\\nemma.concordance(\"surprize\")\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "注意\n",
    "\n",
    "在第1章中，我们演示了如何使用text1.concordance()命令对text1这样的文本进行索引。\n",
    "然而，这是假设你正在使用由from nltk.book import *导入的9 个文本之一。\n",
    "现在你开始研究nltk.corpus中的数据，像前面的例子一样，\n",
    "你必须采用以下语句对来处理索引和第1章中的其它任务：\n",
    "\n",
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "emma.concordance(\"surprize\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "                                       a b c a\n",
      "                                 a b c a \n"
     ]
    }
   ],
   "source": [
    "nltk.Text(['a','b','c','a']).concordance('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "#写一个简短的程序，通过循环遍历前面列出的gutenberg文件标识符列表相应的fileid，然后计算统计每个文本\n",
    "\n",
    "#这个程序显示每个文本的三个统计量：平均词长、平均句子长度和本文中每个词出现的平均次数\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nraw()函数给我们没有进行过任何语言学处理的文件的内容\\nlen(gutenberg.raw('blake-poems.txt'))告诉我们文本中出现的字符个数，包括词之间的空格\\nsents()函数把文本划分成句子，其中每一个句子是一个单词列表\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "raw()函数给我们没有进行过任何语言学处理的文件的内容\n",
    "len(gutenberg.raw('blake-poems.txt'))告诉我们文本中出现的字符个数，包括词之间的空格\n",
    "sents()函数把文本划分成句子，其中每一个句子是一个单词列表\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n网络和聊天文本\\n\\nfrom nltk.corpus import webtext\\nfor fileid in webtext.fileids():\\n    print(fileid, webtext.raw(fileid)[:65], '...')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "网络和聊天文本\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n布朗语料库\\n\\n\\n布朗语料库是第一个百万词级的英语电子语料库的，由布朗大学于1961 年创建。\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "布朗语料库\n",
    "\n",
    "\n",
    "布朗语料库是第一个百万词级的英语电子语料库的，由布朗大学于1961 年创建。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "可以将语料库作为单词  列表或者句子列表  来访问,每个句子本身也是一个单词列表\n",
    "'''\n",
    "\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Romantic', 'news', 'concerns', 'Mrs.', 'Joan', ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(fileids=['ca16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=['news', 'editorial', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "'''\n",
    "布朗语料库是一个研究文体之间的系统性差异——一种叫做文体学的语言学研究——很方便的资源。让我们来比较不同文体中的情态动词的用法。第一步是产生特定文体的计数。记住做下面的实验之前要import nltk：\n",
    "'''\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')#设置end = '  '以让print函数将其输出放在单独的一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n使用NLTK 提供的带条件的频率分布函数\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用NLTK 提供的带条件的频率分布函数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "...           (genre, word)\n",
    "...           for genre in brown.categories()\n",
    "...           for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ConditionalFreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd2=nltk.ConditionalFreqDist((len(word),word) for word in brown.words(categories=\"news\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Contributions  Distributive \n",
      "10             0             0 \n",
      "11             0             0 \n",
      "12             0             1 \n",
      "13             1             0 \n",
      "14             0             0 \n",
      "15             0             0 \n",
      "16             0             0 \n"
     ]
    }
   ],
   "source": [
    "cfd2.tabulate(conditions=[10,11,12,13,14,15,16],samples=['Contributions','Distributive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n示例\\t描述\\nfileids()\\t语料库中的文件\\nfileids([categories])\\t这些分类对应的语料库中的文件\\ncategories()\\t语料库中的分类\\ncategories([fileids])\\t这些文件对应的语料库中的分类\\nraw()\\t语料库的原始内容\\nraw(fileids=[f1,f2,f3])\\t指定文件的原始内容\\nraw(categories=[c1,c2])\\t指定分类的原始内容\\nwords()\\t整个语料库中的词汇\\nwords(fileids=[f1,f2,f3])\\t指定文件中的词汇\\nwords(categories=[c1,c2])\\t指定分类中的词汇\\nsents()\\t整个语料库中的句子\\nsents(fileids=[f1,f2,f3])\\t指定文件中的句子\\nsents(categories=[c1,c2])\\t指定分类中的句子\\nabspath(fileid)\\t指定文件在磁盘上的位置\\nencoding(fileid)\\t文件的编码（如果知道的话）\\nopen(fileid)\\t打开指定语料库文件的文件流\\nroot\\t本地安装的语料库根目录的路径\\nreadme()\\t语料库的README 文件的内容\\n\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "示例\t描述\n",
    "fileids()\t语料库中的文件\n",
    "fileids([categories])\t这些分类对应的语料库中的文件\n",
    "categories()\t语料库中的分类\n",
    "categories([fileids])\t这些文件对应的语料库中的分类\n",
    "raw()\t语料库的原始内容\n",
    "raw(fileids=[f1,f2,f3])\t指定文件的原始内容\n",
    "raw(categories=[c1,c2])\t指定分类的原始内容\n",
    "words()\t整个语料库中的词汇\n",
    "words(fileids=[f1,f2,f3])\t指定文件中的词汇\n",
    "words(categories=[c1,c2])\t指定分类中的词汇\n",
    "sents()\t整个语料库中的句子\n",
    "sents(fileids=[f1,f2,f3])\t指定文件中的句子\n",
    "sents(categories=[c1,c2])\t指定分类中的句子\n",
    "abspath(fileid)\t指定文件在磁盘上的位置\n",
    "encoding(fileid)\t文件的编码（如果知道的话）\n",
    "open(fileid)\t打开指定语料库文件的文件流\n",
    "root\t本地安装的语料库根目录的路径\n",
    "readme()\t语料库的README 文件的内容\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BROWN CORPUS\\n\\nA Standard Corpus of Present-Day Edited American\\nEnglish, for use with Digital Computers.\\n\\nby W. N. Francis and H. Kucera (1964)\\nDepartment of Linguistics, Brown University\\nProvidence, Rhode Island, USA\\n\\nRevised 1971, Revised and Amplified 1979\\n\\nhttp://www.hit.uib.no/icame/brown/bcm.html\\n\\nDistributed with the permission of the copyright holder,\\nredistribution permitted.\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n加载你自己的语料库\\n\\nPlaintextCorpusReader\\n检查你的文件在文件系统中的位置\\n不管是什么位置，将变量corpus_root[1]的值设置为这个目录\\nPlaintextCorpusReader初始化函数[2]的第二个参数可以是一个如['a.txt', 'test/b.txt']这样的fileids列表，或者一个匹配所有fileids 的模式，如'[abc]/.*\\\\.txt'（关于正则表达式的信息见3.4节）\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "加载你自己的语料库\n",
    "\n",
    "PlaintextCorpusReader\n",
    "检查你的文件在文件系统中的位置\n",
    "不管是什么位置，将变量corpus_root[1]的值设置为这个目录\n",
    "PlaintextCorpusReader初始化函数[2]的第二个参数可以是一个如['a.txt', 'test/b.txt']这样的fileids列表，或者一个匹配所有fileids 的模式，如'[abc]/.*\\.txt'（关于正则表达式的信息见3.4节）\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus=\".\\mycorpus\\origin\"\n",
    "corpus_root=mycorpus\n",
    "file_pattern=\".*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto_comments_orig.txt',\n",
       " 'food_comments_orig.txt',\n",
       " 'news_orig.txt',\n",
       " 'weibo_orig.txt']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['怀化什么时候能有自己的马拉松赛', '？', '_都市生活_怀化新闻网', ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.words(\"news_orig.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n条件频率分布\\n条件频率分布是频率分布的集合，每个频率分布有一个不同的“条件”。这个条件通常是文本的类别。\\n\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "条件频率分布\n",
    "条件频率分布是频率分布的集合，每个频率分布有一个不同的“条件”。这个条件通常是文本的类别。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n频率分布计算观察到的事件，如文本中出现的词汇。\\n\\n条件频率分布需要给每个事件关联一个条件。所以不是处理一个单词词序列[1]，我们必须处理的是一个配对序列\\n\\n\\n每个配对的形式是：(条件, 事件)\\n\\n\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "频率分布计算观察到的事件，如文本中出现的词汇。\n",
    "\n",
    "条件频率分布需要给每个事件关联一个条件。所以不是处理一个单词词序列[1]，我们必须处理的是一个配对序列\n",
    "\n",
    "\n",
    "每个配对的形式是：(条件, 事件)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "按文体计数词汇\n",
    "'''\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "'''\n",
    "对于每个文体[2]，我们遍历文体中的每个词[3]，以产生文体与词的配对\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_word=[(genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adventure', 'Dan'),\n",
       " ('adventure', 'Morgan'),\n",
       " ('adventure', 'told'),\n",
       " ('adventure', 'himself'),\n",
       " ('adventure', 'he'),\n",
       " ('adventure', 'would'),\n",
       " ('adventure', 'forget'),\n",
       " ('adventure', 'Ann'),\n",
       " ('adventure', 'Turner'),\n",
       " ('adventure', '.'),\n",
       " ('adventure', 'He'),\n",
       " ('adventure', 'was'),\n",
       " ('adventure', 'well'),\n",
       " ('adventure', 'rid'),\n",
       " ('adventure', 'of'),\n",
       " ('adventure', 'her'),\n",
       " ('adventure', '.'),\n",
       " ('adventure', 'He'),\n",
       " ('adventure', 'certainly'),\n",
       " ('adventure', \"didn't\")]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    " cfd = nltk.ConditionalFreqDist(genre_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 14394 samples and 100554 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(cfd['news'])#访问这两个条件，它们每一个都只是一个频率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 8452 samples and 70022 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(cfd['romance'])#访问这两个条件，它们每一个都只是一个频率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n使用双连词生成随机文本\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用双连词生成随机文本\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams()函数接受一个单词列表，并建立一个连续的词对列表。记住，为了能看到结果而不是神秘的\"生成器对象\"，我们需要使用list()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'Fulton'),\n",
       " ('Fulton', 'County'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Jury'),\n",
       " ('Jury', 'said'),\n",
       " ('said', 'Friday'),\n",
       " ('Friday', 'an'),\n",
       " ('an', 'investigation'),\n",
       " ('investigation', 'of'),\n",
       " ('of', \"Atlanta's\"),\n",
       " (\"Atlanta's\", 'recent'),\n",
       " ('recent', 'primary'),\n",
       " ('primary', 'election'),\n",
       " ('election', 'produced'),\n",
       " ('produced', '``'),\n",
       " ('``', 'no'),\n",
       " ('no', 'evidence'),\n",
       " ('evidence', \"''\"),\n",
       " (\"''\", 'that'),\n",
       " ('that', 'any'),\n",
       " ('any', 'irregularities'),\n",
       " ('irregularities', 'took'),\n",
       " ('took', 'place'),\n",
       " ('place', '.')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(brown.sents('ca01')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n示例\\t描述\\ncfdist = ConditionalFreqDist(pairs)\\t从配对列表中创建条件频率分布\\ncfdist.conditions()\\t条件\\ncfdist[condition]\\t此条件下的频率分布\\ncfdist[condition][sample]\\t此条件下给定样本的频率\\ncfdist.tabulate()\\t为条件频率分布制表\\ncfdist.tabulate(samples, conditions)\\t指定样本和条件限制下制表\\ncfdist.plot()\\t为条件频率分布绘图\\ncfdist.plot(samples, conditions)\\t指定样本和条件限制下绘图\\ncfdist1 < cfdist2\\t测试样本在cfdist1中出现次数是否小于在cfdist2中出现次数\\n\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "示例\t描述\n",
    "cfdist = ConditionalFreqDist(pairs)\t从配对列表中创建条件频率分布\n",
    "cfdist.conditions()\t条件\n",
    "cfdist[condition]\t此条件下的频率分布\n",
    "cfdist[condition][sample]\t此条件下给定样本的频率\n",
    "cfdist.tabulate()\t为条件频率分布制表\n",
    "cfdist.tabulate(samples, conditions)\t指定样本和条件限制下制表\n",
    "cfdist.plot()\t为条件频率分布绘图\n",
    "cfdist.plot(samples, conditions)\t指定样本和条件限制下绘图\n",
    "cfdist1 < cfdist2\t测试样本在cfdist1中出现次数是否小于在cfdist2中出现次数\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n文本语料库是一个大型结构化文本的集合。NLTK 包含了许多语料库，如布朗语料库nltk.corpus.brown。\\n有些文本语料库是分类的，例如通过文体或者主题分类；有时候语料库的分类会相互重叠。\\n条件频率分布是一个频率分布的集合，每个分布都有一个不同的条件。它们可以用于通过给定内容或者文体对词的频率计数。\\n行数较多的Python 程序应该使用文本编辑器来输入，保存为.py后缀的文件，并使用import语句来访问。\\nPython 函数允许你将一段特定的代码块与一个名字联系起来，然后重用这些代码想用多少次就用多少次。\\n一些被称为“方法”的函数与一个对象联系在起来，我们使用对象名称跟一个点然后跟方法名称来调用它，就像：x.funct(y)或者word.isalpha()。\\n要想找到一些关于某个变量v的信息，可以在Pyhon交互式解释器中输入help(v)来阅读这一类对象的帮助条目。\\nWordNet是一个面向语义的英语词典，由同义词的集合——或称为同义词集——组成，并且组织成一个网络。\\n默认情况下有些函数是不能使用的，必须使用Python的import语句来访问。\\n\\n'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "文本语料库是一个大型结构化文本的集合。NLTK 包含了许多语料库，如布朗语料库nltk.corpus.brown。\n",
    "有些文本语料库是分类的，例如通过文体或者主题分类；有时候语料库的分类会相互重叠。\n",
    "条件频率分布是一个频率分布的集合，每个分布都有一个不同的条件。它们可以用于通过给定内容或者文体对词的频率计数。\n",
    "行数较多的Python 程序应该使用文本编辑器来输入，保存为.py后缀的文件，并使用import语句来访问。\n",
    "Python 函数允许你将一段特定的代码块与一个名字联系起来，然后重用这些代码想用多少次就用多少次。\n",
    "一些被称为“方法”的函数与一个对象联系在起来，我们使用对象名称跟一个点然后跟方法名称来调用它，就像：x.funct(y)或者word.isalpha()。\n",
    "要想找到一些关于某个变量v的信息，可以在Pyhon交互式解释器中输入help(v)来阅读这一类对象的帮助条目。\n",
    "WordNet是一个面向语义的英语词典，由同义词的集合——或称为同义词集——组成，并且组织成一个网络。\n",
    "默认情况下有些函数是不能使用的，必须使用Python的import语句来访问。\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
