{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n我们怎样用特征扩展上下文无关语法框架，以获得更细粒度的对语法类别和产生式的控制？\\n特征结构的主要形式化属性是什么，我们如何使用它们来计算？\\n我们现在用基于特征的语法能捕捉到什么语言模式和语法结构？\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "我们怎样用特征扩展上下文无关语法框架，以获得更细粒度的对语法类别和产生式的控制？\n",
    "特征结构的主要形式化属性是什么，我们如何使用它们来计算？\n",
    "我们现在用基于特征的语法能捕捉到什么语言模式和语法结构？\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n特征在建立基于规则的语法中的作用\\n\\n>>> kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\\n>>> chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}\\n\\n对象kim和chase有几个共同的特征，\\nCAT（语法类别）和ORTH（正字法，即拼写）。\\n\\n此外，每一个还有更面向语义的特征：\\nkim['REF']意在给出kim的指示物，而chase['REL']给出chase表示的关系。\\n在基于规则的语法上下文中，这样的特征和特征值对被称为特征结构，\\n我们将很快看到它们的替代符号。\\n\\n\\n特征结构包含各种有关语法实体的信息\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "特征在建立基于规则的语法中的作用\n",
    "\n",
    ">>> kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
    ">>> chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}\n",
    "\n",
    "对象kim和chase有几个共同的特征，\n",
    "CAT（语法类别）和ORTH（正字法，即拼写）。\n",
    "\n",
    "此外，每一个还有更面向语义的特征：\n",
    "kim['REF']意在给出kim的指示物，而chase['REL']给出chase表示的关系。\n",
    "在基于规则的语法上下文中，这样的特征和特征值对被称为特征结构，\n",
    "我们将很快看到它们的替代符号。\n",
    "\n",
    "\n",
    "特征结构包含各种有关语法实体的信息\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n进一步增加属性\\n\\n>>> chase[\\'AGT\\'] = \\'sbj\\'\\n>>> chase[\\'PAT\\'] = \\'obj\\'\\n\\n\\n在处理句子Kim chased Lee,我们要“绑定”动词的  施事角色  和  主语，受事角色和宾语\\n下面的例子中，我们做一个简单的假设：在动词直接左侧和右侧的NP分别是主语和宾语\\n>>> sent = \"Kim chased Lee\"\\n>>> tokens = sent.split()\\n>>> lee = {\\'CAT\\': \\'NP\\', \\'ORTH\\': \\'Lee\\', \\'REF\\': \\'l\\'}\\n>>> def lex2fs(word):\\n...     for fs in [kim, lee, chase]:\\n...         if fs[\\'ORTH\\'] == word:\\n...             return fs\\n>>> subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\\n>>> verb[\\'AGT\\'] = subj[\\'REF\\']\\n>>> verb[\\'PAT\\'] = obj[\\'REF\\']\\n>>> for k in [\\'ORTH\\', \\'REL\\', \\'AGT\\', \\'PAT\\']:\\n...     print(\"%-5s => %s\" % (k, verb[k]))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "进一步增加属性\n",
    "\n",
    ">>> chase['AGT'] = 'sbj'\n",
    ">>> chase['PAT'] = 'obj'\n",
    "\n",
    "\n",
    "在处理句子Kim chased Lee,我们要“绑定”动词的  施事角色  和  主语，受事角色和宾语\n",
    "下面的例子中，我们做一个简单的假设：在动词直接左侧和右侧的NP分别是主语和宾语\n",
    ">>> sent = \"Kim chased Lee\"\n",
    ">>> tokens = sent.split()\n",
    ">>> lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
    ">>> def lex2fs(word):\n",
    "...     for fs in [kim, lee, chase]:\n",
    "...         if fs['ORTH'] == word:\n",
    "...             return fs\n",
    ">>> subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n",
    ">>> verb['AGT'] = subj['REF']\n",
    ">>> verb['PAT'] = obj['REF']\n",
    ">>> for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
    "...     print(\"%-5s => %s\" % (k, verb[k]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.1 句法协议\\n\\n\\n英语规则动词的协议范式\\n\\n单数\\t复数\\n第一人称\\tI run\\twe run\\n第二人称\\tyou run\\tyou run\\n第三人称\\the/she/it runs\\tthey run\\n\\n\\n我们用3 作为第三人称的缩写，SG 表示单数，PL 表示复数\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.1 句法协议\n",
    "\n",
    "\n",
    "英语规则动词的协议范式\n",
    "\n",
    "单数\t复数\n",
    "第一人称\tI run\twe run\n",
    "第二人称\tyou run\tyou run\n",
    "第三人称\the/she/it runs\tthey run\n",
    "\n",
    "\n",
    "我们用3 作为第三人称的缩写，SG 表示单数，PL 表示复数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1.2 使用属性和约束\\n\\n注意一个句法类别可以有多个特征，例如V[TENSE=pres, NUM=pl]。在一般情况下，我们喜欢多少特征就可以添加多少。\\n\\n\\n>>> nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')\\n% start S    #最后的细节是语句%start S。这个“指令”告诉分析器以S作为文法的开始符号\\n# ###################\\n# Grammar Productions\\n# ###################\\n# S expansion productions\\nS -> NP[NUM=?n] VP[NUM=?n]\\n# NP expansion productions\\nNP[NUM=?n] -> N[NUM=?n]\\nNP[NUM=?n] -> PropN[NUM=?n]\\nNP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\\nNP[NUM=pl] -> N[NUM=pl]\\n# VP expansion productions\\nVP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\\nVP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\\n# ###################\\n# Lexical Productions\\n# ###################\\nDet[NUM=sg] -> 'this' | 'every'\\nDet[NUM=pl] -> 'these' | 'all'\\nDet -> 'the' | 'some' | 'several'\\nPropN[NUM=sg]-> 'Kim' | 'Jody'\\nN[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\\nN[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\\nIV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\\nTV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\\nIV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\\nTV[TENSE=pres, NUM=pl] -> 'see' | 'like'\\nIV[TENSE=past] -> 'disappeared' | 'walked'\\nTV[TENSE=past] -> 'saw' | 'liked'\\n\\n最后的细节是语句%start S。这个“指令”告诉分析器以S作为文法的开始符号\\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.2 使用属性和约束\n",
    "\n",
    "注意一个句法类别可以有多个特征，例如V[TENSE=pres, NUM=pl]。在一般情况下，我们喜欢多少特征就可以添加多少。\n",
    "\n",
    "\n",
    ">>> nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')\n",
    "% start S    #最后的细节是语句%start S。这个“指令”告诉分析器以S作为文法的开始符号\n",
    "# ###################\n",
    "# Grammar Productions\n",
    "# ###################\n",
    "# S expansion productions\n",
    "S -> NP[NUM=?n] VP[NUM=?n]\n",
    "# NP expansion productions\n",
    "NP[NUM=?n] -> N[NUM=?n]\n",
    "NP[NUM=?n] -> PropN[NUM=?n]\n",
    "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
    "NP[NUM=pl] -> N[NUM=pl]\n",
    "# VP expansion productions\n",
    "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
    "# ###################\n",
    "# Lexical Productions\n",
    "# ###################\n",
    "Det[NUM=sg] -> 'this' | 'every'\n",
    "Det[NUM=pl] -> 'these' | 'all'\n",
    "Det -> 'the' | 'some' | 'several'\n",
    "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
    "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
    "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\n",
    "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
    "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
    "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
    "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
    "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
    "TV[TENSE=past] -> 'saw' | 'liked'\n",
    "\n",
    "最后的细节是语句%start S。这个“指令”告诉分析器以S作为文法的开始符号\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"一般情况下，即使我们正在尝试开发很小的语法，把产生式放在一个文件中我们可以编辑、测试和修改是很方便的。我们将1.1以NLTK 的数据格式保存为文件'feat0.fcfg'。你可以使用nltk.data.load()制作你自己的副本进行进一步的实验。\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''一般情况下，即使我们正在尝试开发很小的语法，把产生式放在一个文件中我们可以编辑、测试和修改是很方便的。我们将1.1以NLTK 的数据格式保存为文件'feat0.fcfg'。你可以使用nltk.data.load()制作你自己的副本进行进一步的实验。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n]\n",
      "NP[NUM=?n] -> PropN[NUM=?n]\n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl]\n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.show_cfg('feat0.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "调用分析器的parse()方法将迭代生成的分析树；如果文法无法分析输入，trees将为空，并将会包含一个或多个分析树，取决于输入是否有句法歧义。\n",
    "'''\n",
    "\n",
    "tokens = 'Kim likes children'.split()\n",
    "from nltk import load_parser \n",
    "cp = load_parser('grammars/book_grammars/feat0.fcfg', trace=2)\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "最后，我们可以检查生成的分析树（在这种情况下，只有一个）。\n",
    "'''\n",
    "trees=cp.parse(tokens)\n",
    "for tree in trees: print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.3 术语\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.3 术语\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n到目前为止，我们只看到像sg和pl这样的特征值。这些简单的值通常被称为原子——也就是，它们不能被分解成更小的部分\\n\\n原子值的一种特殊情况是布尔值，也就是说，值仅仅指定一个属性是真还是假。\\n\\n\\n我们说过分配“特征注释”给句法类别。\\n\\n表示整个类别的更激进的做法——也就是，\\n非终结符加注释——作为一个特征的绑定。例如，N[NUM=sg]包含词性信息，可以表示为POS=N。因此，这个类别的替代符号是[POS=N, NUM=sg]。\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "到目前为止，我们只看到像sg和pl这样的特征值。这些简单的值通常被称为原子——也就是，它们不能被分解成更小的部分\n",
    "\n",
    "原子值的一种特殊情况是布尔值，也就是说，值仅仅指定一个属性是真还是假。\n",
    "\n",
    "\n",
    "我们说过分配“特征注释”给句法类别。\n",
    "\n",
    "表示整个类别的更激进的做法——也就是，\n",
    "非终结符加注释——作为一个特征的绑定。例如，N[NUM=sg]包含词性信息，可以表示为POS=N。因此，这个类别的替代符号是[POS=N, NUM=sg]。\n",
    "\n",
    "\n",
    "除了原子值特征，特征可能需要本身就是特征结构的值。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2 处理特征结构\\n\\nNLTK中的特征结构使用构造函数FeatStruct()声明。原子特征值可以是字符串或整数。\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2 处理特征结构\n",
    "\n",
    "NLTK中的特征结构使用构造函数FeatStruct()声明。原子特征值可以是字符串或整数。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
